{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cosin\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "import keras.backend as K\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(filepath, X):\n",
    "    with open(filepath) as file:\n",
    "        array = [line.strip() for line in file]\n",
    "    full_size = int(array[0].split(' ')[0])\n",
    "    features = int(array[0].split(' ')[1])\n",
    "    classes = int(array[0].split(' ')[2])\n",
    "    sample_size = 0\n",
    "    for i in range(min(full_size*2, 2000)):\n",
    "        s = array[i].split(' ')\n",
    "        if len(s) > 10:\n",
    "            sample_size += 1\n",
    "            X.append(s)\n",
    "    return sample_size, features, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFullFile(filepath, X):\n",
    "    with open(filepath) as file:\n",
    "        array = [line.strip() for line in file]\n",
    "    full_size = int(array[0].split(' ')[0])\n",
    "    features = int(array[0].split(' ')[1])\n",
    "    classes = int(array[0].split(' ')[2])\n",
    "    sample_size = 0\n",
    "    for i in range(full_size*2):\n",
    "        s = array[i].split(' ')\n",
    "        if len(s) > 10:\n",
    "            sample_size += 1\n",
    "            X.append(s)\n",
    "    return sample_size, features, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_4 = [] # Навальный\n",
    "X_5 = [] # Путин"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_4, features, _ = readFile('trainFull/train4.data', X_4)\n",
    "size_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_5, _, features = readFile('trainFull/train5.data', X_5)\n",
    "size_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_4 = np.array(X_4).astype(float)\n",
    "X_5 = np.array(X_5).astype(float)\n",
    "X_4 = (X_4+1)/2\n",
    "X_5 = (X_5+1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_4 = np.zeros(1000)\n",
    "y_5 = np.ones(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 7282)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.vstack((X_4, X_5))\n",
    "y = np.append(y_4, y_5)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_4_full = [] # Навальный\n",
    "X_5_full = [] # Путин"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1998"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_4, _, _ = readFullFile('trainFull/train4.data', X_4_full)\n",
    "size_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_5, _, _ = readFullFile('trainFull/train5.data', X_5_full)\n",
    "size_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_4_full = np.array(X_4_full).astype(float)\n",
    "X_5_full = np.array(X_5_full).astype(float)\n",
    "X_4_full = (X_4_full+1)/2\n",
    "X_5_full = (X_5_full+1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6998, 7282)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_full = np.vstack((X_4_full, X_5_full))\n",
    "y_full = np.append(np.zeros(X_4_full.shape[0]), np.ones(X_5_full.shape[0]))\n",
    "X_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Cumulative explained variance')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkawfG2JG6gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f301c803358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca = PCA(n_components=7000)\n",
    "pca.fit(X_full)\n",
    "\n",
    "plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "xlabel('Number of components')\n",
    "ylabel('Cumulative explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 7282)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2400 samples, validate on 1200 samples\n",
      "Epoch 1/80\n",
      "2400/2400 [==============================] - 4s 2ms/step - loss: 0.6940 - acc: 0.4608 - val_loss: 0.6902 - val_acc: 0.5300\n",
      "Epoch 2/80\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 0.6872 - acc: 0.5888 - val_loss: 0.6837 - val_acc: 0.6475\n",
      "Epoch 3/80\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 0.6811 - acc: 0.6812 - val_loss: 0.6776 - val_acc: 0.7300\n",
      "Epoch 4/80\n",
      "2400/2400 [==============================] - 0s 107us/step - loss: 0.6752 - acc: 0.7288 - val_loss: 0.6716 - val_acc: 0.7608\n",
      "Epoch 5/80\n",
      "2400/2400 [==============================] - 0s 98us/step - loss: 0.6694 - acc: 0.7508 - val_loss: 0.6655 - val_acc: 0.7642\n",
      "Epoch 6/80\n",
      "2400/2400 [==============================] - 0s 108us/step - loss: 0.6634 - acc: 0.7608 - val_loss: 0.6592 - val_acc: 0.7750\n",
      "Epoch 7/80\n",
      "2400/2400 [==============================] - 0s 101us/step - loss: 0.6571 - acc: 0.7700 - val_loss: 0.6523 - val_acc: 0.7792\n",
      "Epoch 8/80\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 0.6503 - acc: 0.7750 - val_loss: 0.6450 - val_acc: 0.7808\n",
      "Epoch 9/80\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 0.6430 - acc: 0.7871 - val_loss: 0.6373 - val_acc: 0.7800\n",
      "Epoch 10/80\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 0.6354 - acc: 0.7883 - val_loss: 0.6292 - val_acc: 0.7858\n",
      "Epoch 11/80\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 0.6275 - acc: 0.7854 - val_loss: 0.6208 - val_acc: 0.7867\n",
      "Epoch 12/80\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.6193 - acc: 0.7871 - val_loss: 0.6121 - val_acc: 0.7883\n",
      "Epoch 13/80\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 0.6109 - acc: 0.7871 - val_loss: 0.6032 - val_acc: 0.7875\n",
      "Epoch 14/80\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 0.6022 - acc: 0.7904 - val_loss: 0.5941 - val_acc: 0.7875\n",
      "Epoch 15/80\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 0.5934 - acc: 0.7908 - val_loss: 0.5849 - val_acc: 0.7892\n",
      "Epoch 16/80\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 0.5845 - acc: 0.7929 - val_loss: 0.5757 - val_acc: 0.7892\n",
      "Epoch 17/80\n",
      "2400/2400 [==============================] - 0s 103us/step - loss: 0.5757 - acc: 0.7892 - val_loss: 0.5665 - val_acc: 0.7900\n",
      "Epoch 18/80\n",
      "2400/2400 [==============================] - 0s 96us/step - loss: 0.5670 - acc: 0.7862 - val_loss: 0.5574 - val_acc: 0.7908\n",
      "Epoch 19/80\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 0.5584 - acc: 0.7875 - val_loss: 0.5484 - val_acc: 0.7925\n",
      "Epoch 20/80\n",
      "2400/2400 [==============================] - 0s 81us/step - loss: 0.5499 - acc: 0.7871 - val_loss: 0.5397 - val_acc: 0.7917\n",
      "Epoch 21/80\n",
      "2400/2400 [==============================] - 0s 111us/step - loss: 0.5416 - acc: 0.7875 - val_loss: 0.5311 - val_acc: 0.7925\n",
      "Epoch 22/80\n",
      "2400/2400 [==============================] - 0s 94us/step - loss: 0.5336 - acc: 0.7888 - val_loss: 0.5229 - val_acc: 0.7933\n",
      "Epoch 23/80\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 0.5258 - acc: 0.7896 - val_loss: 0.5149 - val_acc: 0.7950\n",
      "Epoch 24/80\n",
      "2400/2400 [==============================] - 0s 78us/step - loss: 0.5183 - acc: 0.7933 - val_loss: 0.5073 - val_acc: 0.7950\n",
      "Epoch 25/80\n",
      "2400/2400 [==============================] - 0s 102us/step - loss: 0.5112 - acc: 0.7933 - val_loss: 0.5002 - val_acc: 0.7958\n",
      "Epoch 26/80\n",
      "2400/2400 [==============================] - 0s 79us/step - loss: 0.5044 - acc: 0.7917 - val_loss: 0.4935 - val_acc: 0.7975\n",
      "Epoch 27/80\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 0.4981 - acc: 0.7950 - val_loss: 0.4870 - val_acc: 0.7983\n",
      "Epoch 28/80\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.4921 - acc: 0.7963 - val_loss: 0.4810 - val_acc: 0.7975\n",
      "Epoch 29/80\n",
      "2400/2400 [==============================] - 0s 99us/step - loss: 0.4865 - acc: 0.7971 - val_loss: 0.4754 - val_acc: 0.7975\n",
      "Epoch 30/80\n",
      "2400/2400 [==============================] - 0s 92us/step - loss: 0.4811 - acc: 0.7963 - val_loss: 0.4702 - val_acc: 0.7983\n",
      "Epoch 31/80\n",
      "2400/2400 [==============================] - 0s 104us/step - loss: 0.4762 - acc: 0.7958 - val_loss: 0.4654 - val_acc: 0.8008\n",
      "Epoch 32/80\n",
      "2400/2400 [==============================] - 0s 120us/step - loss: 0.4715 - acc: 0.7992 - val_loss: 0.4609 - val_acc: 0.8042\n",
      "Epoch 33/80\n",
      "2400/2400 [==============================] - 0s 105us/step - loss: 0.4672 - acc: 0.7987 - val_loss: 0.4567 - val_acc: 0.8075\n",
      "Epoch 34/80\n",
      "2400/2400 [==============================] - 0s 101us/step - loss: 0.4630 - acc: 0.8017 - val_loss: 0.4527 - val_acc: 0.8042\n",
      "Epoch 35/80\n",
      "2400/2400 [==============================] - 0s 113us/step - loss: 0.4591 - acc: 0.8017 - val_loss: 0.4490 - val_acc: 0.8075\n",
      "Epoch 36/80\n",
      "2400/2400 [==============================] - 0s 82us/step - loss: 0.4555 - acc: 0.8025 - val_loss: 0.4456 - val_acc: 0.8108\n",
      "Epoch 37/80\n",
      "2400/2400 [==============================] - 0s 79us/step - loss: 0.4522 - acc: 0.8042 - val_loss: 0.4424 - val_acc: 0.8108\n",
      "Epoch 38/80\n",
      "2400/2400 [==============================] - 0s 110us/step - loss: 0.4489 - acc: 0.8038 - val_loss: 0.4395 - val_acc: 0.8150\n",
      "Epoch 39/80\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.4459 - acc: 0.8075 - val_loss: 0.4367 - val_acc: 0.8158\n",
      "Epoch 40/80\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 0.4430 - acc: 0.8058 - val_loss: 0.4342 - val_acc: 0.8167\n",
      "Epoch 41/80\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 0.4403 - acc: 0.8087 - val_loss: 0.4318 - val_acc: 0.8200\n",
      "Epoch 42/80\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 0.4377 - acc: 0.8100 - val_loss: 0.4295 - val_acc: 0.8208\n",
      "Epoch 43/80\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 0.4354 - acc: 0.8096 - val_loss: 0.4275 - val_acc: 0.8192\n",
      "Epoch 44/80\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 0.4332 - acc: 0.8104 - val_loss: 0.4255 - val_acc: 0.8183\n",
      "Epoch 45/80\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 0.4310 - acc: 0.8108 - val_loss: 0.4238 - val_acc: 0.8175\n",
      "Epoch 46/80\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 0.4290 - acc: 0.8133 - val_loss: 0.4221 - val_acc: 0.8183\n",
      "Epoch 47/80\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 0.4271 - acc: 0.8121 - val_loss: 0.4205 - val_acc: 0.8183\n",
      "Epoch 48/80\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 0.4254 - acc: 0.8108 - val_loss: 0.4191 - val_acc: 0.8192\n",
      "Epoch 49/80\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 0.4236 - acc: 0.8133 - val_loss: 0.4177 - val_acc: 0.8183\n",
      "Epoch 50/80\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 0.4221 - acc: 0.8138 - val_loss: 0.4164 - val_acc: 0.8183\n",
      "Epoch 51/80\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 0.4205 - acc: 0.8129 - val_loss: 0.4152 - val_acc: 0.8200\n",
      "Epoch 52/80\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 0.4190 - acc: 0.8129 - val_loss: 0.4141 - val_acc: 0.8192\n",
      "Epoch 53/80\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 0.4176 - acc: 0.8125 - val_loss: 0.4132 - val_acc: 0.8200\n",
      "Epoch 54/80\n",
      "2400/2400 [==============================] - 0s 64us/step - loss: 0.4162 - acc: 0.8150 - val_loss: 0.4122 - val_acc: 0.8200\n",
      "Epoch 55/80\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 0.4147 - acc: 0.8142 - val_loss: 0.4112 - val_acc: 0.8208\n",
      "Epoch 56/80\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 0.4135 - acc: 0.8142 - val_loss: 0.4103 - val_acc: 0.8208\n",
      "Epoch 57/80\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 0.4124 - acc: 0.8158 - val_loss: 0.4095 - val_acc: 0.8217\n",
      "Epoch 58/80\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 0.4111 - acc: 0.8154 - val_loss: 0.4087 - val_acc: 0.8208\n",
      "Epoch 59/80\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 0.4099 - acc: 0.8146 - val_loss: 0.4081 - val_acc: 0.8167\n",
      "Epoch 60/80\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 0.4089 - acc: 0.8192 - val_loss: 0.4073 - val_acc: 0.8192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/80\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 0.4077 - acc: 0.8175 - val_loss: 0.4068 - val_acc: 0.8183\n",
      "Epoch 62/80\n",
      "2400/2400 [==============================] - 0s 65us/step - loss: 0.4069 - acc: 0.8200 - val_loss: 0.4062 - val_acc: 0.8200\n",
      "Epoch 63/80\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 0.4058 - acc: 0.8187 - val_loss: 0.4054 - val_acc: 0.8225\n",
      "Epoch 64/80\n",
      "2400/2400 [==============================] - 0s 65us/step - loss: 0.4049 - acc: 0.8192 - val_loss: 0.4049 - val_acc: 0.8217\n",
      "Epoch 65/80\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 0.4039 - acc: 0.8183 - val_loss: 0.4043 - val_acc: 0.8225\n",
      "Epoch 66/80\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 0.4030 - acc: 0.8175 - val_loss: 0.4039 - val_acc: 0.8208\n",
      "Epoch 67/80\n",
      "2400/2400 [==============================] - 0s 87us/step - loss: 0.4022 - acc: 0.8200 - val_loss: 0.4034 - val_acc: 0.8208\n",
      "Epoch 68/80\n",
      "2400/2400 [==============================] - 0s 106us/step - loss: 0.4012 - acc: 0.8187 - val_loss: 0.4031 - val_acc: 0.8217\n",
      "Epoch 69/80\n",
      "2400/2400 [==============================] - 0s 81us/step - loss: 0.4006 - acc: 0.8208 - val_loss: 0.4027 - val_acc: 0.8217\n",
      "Epoch 70/80\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 0.3997 - acc: 0.8221 - val_loss: 0.4023 - val_acc: 0.8217\n",
      "Epoch 71/80\n",
      "2400/2400 [==============================] - 0s 65us/step - loss: 0.3988 - acc: 0.8229 - val_loss: 0.4018 - val_acc: 0.8200\n",
      "Epoch 72/80\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 0.3981 - acc: 0.8196 - val_loss: 0.4015 - val_acc: 0.8217\n",
      "Epoch 73/80\n",
      "2400/2400 [==============================] - 0s 100us/step - loss: 0.3975 - acc: 0.8225 - val_loss: 0.4011 - val_acc: 0.8208\n",
      "Epoch 74/80\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 0.3966 - acc: 0.8208 - val_loss: 0.4010 - val_acc: 0.8217\n",
      "Epoch 75/80\n",
      "2400/2400 [==============================] - 0s 100us/step - loss: 0.3962 - acc: 0.8233 - val_loss: 0.4005 - val_acc: 0.8192\n",
      "Epoch 76/80\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 0.3953 - acc: 0.8221 - val_loss: 0.4002 - val_acc: 0.8208\n",
      "Epoch 77/80\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 0.3947 - acc: 0.8229 - val_loss: 0.3999 - val_acc: 0.8225\n",
      "Epoch 78/80\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 0.3941 - acc: 0.8229 - val_loss: 0.3996 - val_acc: 0.8200\n",
      "Epoch 79/80\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.3934 - acc: 0.8233 - val_loss: 0.3994 - val_acc: 0.8217\n",
      "Epoch 80/80\n",
      "2400/2400 [==============================] - 0s 111us/step - loss: 0.3928 - acc: 0.8229 - val_loss: 0.3993 - val_acc: 0.8233\n",
      "198/198 [==============================] - 0s 155us/step\n",
      "3200/3200 [==============================] - 0s 135us/step\n",
      "Train on 2400 samples, validate on 1200 samples\n",
      "Epoch 1/80\n",
      "2400/2400 [==============================] - 5s 2ms/step - loss: 0.6830 - acc: 0.5721 - val_loss: 0.6817 - val_acc: 0.5842\n",
      "Epoch 2/80\n",
      "2400/2400 [==============================] - 0s 122us/step - loss: 0.6761 - acc: 0.6154 - val_loss: 0.6749 - val_acc: 0.6317\n",
      "Epoch 3/80\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 0.6691 - acc: 0.6508 - val_loss: 0.6679 - val_acc: 0.6667\n",
      "Epoch 4/80\n",
      "2400/2400 [==============================] - 0s 123us/step - loss: 0.6620 - acc: 0.7004 - val_loss: 0.6607 - val_acc: 0.6942\n",
      "Epoch 5/80\n",
      "2400/2400 [==============================] - 0s 107us/step - loss: 0.6547 - acc: 0.7225 - val_loss: 0.6533 - val_acc: 0.7242\n",
      "Epoch 6/80\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.6471 - acc: 0.7412 - val_loss: 0.6454 - val_acc: 0.7500\n",
      "Epoch 7/80\n",
      "2400/2400 [==============================] - 0s 107us/step - loss: 0.6391 - acc: 0.7558 - val_loss: 0.6371 - val_acc: 0.7533\n",
      "Epoch 8/80\n",
      "2400/2400 [==============================] - 0s 98us/step - loss: 0.6304 - acc: 0.7708 - val_loss: 0.6282 - val_acc: 0.7683\n",
      "Epoch 9/80\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.6213 - acc: 0.7762 - val_loss: 0.6188 - val_acc: 0.7733\n",
      "Epoch 10/80\n",
      "2400/2400 [==============================] - 0s 112us/step - loss: 0.6117 - acc: 0.7775 - val_loss: 0.6090 - val_acc: 0.7708\n",
      "Epoch 11/80\n",
      "2400/2400 [==============================] - 0s 102us/step - loss: 0.6018 - acc: 0.7800 - val_loss: 0.5988 - val_acc: 0.7792\n",
      "Epoch 12/80\n",
      "2400/2400 [==============================] - 0s 91us/step - loss: 0.5917 - acc: 0.7787 - val_loss: 0.5885 - val_acc: 0.7808\n",
      "Epoch 13/80\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 0.5816 - acc: 0.7812 - val_loss: 0.5783 - val_acc: 0.7808\n",
      "Epoch 14/80\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.5717 - acc: 0.7775 - val_loss: 0.5683 - val_acc: 0.7842\n",
      "Epoch 15/80\n",
      "2400/2400 [==============================] - 0s 92us/step - loss: 0.5619 - acc: 0.7812 - val_loss: 0.5584 - val_acc: 0.7842\n",
      "Epoch 16/80\n",
      "2400/2400 [==============================] - 0s 127us/step - loss: 0.5524 - acc: 0.7846 - val_loss: 0.5489 - val_acc: 0.7850\n",
      "Epoch 17/80\n",
      "2400/2400 [==============================] - 0s 112us/step - loss: 0.5433 - acc: 0.7850 - val_loss: 0.5397 - val_acc: 0.7858\n",
      "Epoch 18/80\n",
      "2400/2400 [==============================] - 0s 79us/step - loss: 0.5345 - acc: 0.7833 - val_loss: 0.5308 - val_acc: 0.7875\n",
      "Epoch 19/80\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.5263 - acc: 0.7850 - val_loss: 0.5224 - val_acc: 0.7883\n",
      "Epoch 20/80\n",
      "2400/2400 [==============================] - 0s 132us/step - loss: 0.5183 - acc: 0.7875 - val_loss: 0.5144 - val_acc: 0.7917\n",
      "Epoch 21/80\n",
      "2400/2400 [==============================] - 0s 105us/step - loss: 0.5107 - acc: 0.7900 - val_loss: 0.5067 - val_acc: 0.7933\n",
      "Epoch 22/80\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 0.5037 - acc: 0.7867 - val_loss: 0.4995 - val_acc: 0.7975\n",
      "Epoch 23/80\n",
      "2400/2400 [==============================] - 0s 79us/step - loss: 0.4969 - acc: 0.7900 - val_loss: 0.4927 - val_acc: 0.8000\n",
      "Epoch 24/80\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 0.4906 - acc: 0.7896 - val_loss: 0.4864 - val_acc: 0.8008\n",
      "Epoch 25/80\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.4847 - acc: 0.7954 - val_loss: 0.4804 - val_acc: 0.8033\n",
      "Epoch 26/80\n",
      "2400/2400 [==============================] - 0s 106us/step - loss: 0.4793 - acc: 0.7962 - val_loss: 0.4748 - val_acc: 0.8000\n",
      "Epoch 27/80\n",
      "2400/2400 [==============================] - 0s 127us/step - loss: 0.4742 - acc: 0.7975 - val_loss: 0.4697 - val_acc: 0.8008\n",
      "Epoch 28/80\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.4694 - acc: 0.7975 - val_loss: 0.4649 - val_acc: 0.7992\n",
      "Epoch 29/80\n",
      "2400/2400 [==============================] - ETA: 0s - loss: 0.4667 - acc: 0.801 - 0s 113us/step - loss: 0.4649 - acc: 0.8013 - val_loss: 0.4604 - val_acc: 0.8000\n",
      "Epoch 30/80\n",
      "2400/2400 [==============================] - 0s 108us/step - loss: 0.4608 - acc: 0.8017 - val_loss: 0.4562 - val_acc: 0.8017\n",
      "Epoch 31/80\n",
      "2400/2400 [==============================] - 0s 96us/step - loss: 0.4570 - acc: 0.8000 - val_loss: 0.4523 - val_acc: 0.8033\n",
      "Epoch 32/80\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.4534 - acc: 0.8033 - val_loss: 0.4486 - val_acc: 0.8042\n",
      "Epoch 33/80\n",
      "2400/2400 [==============================] - 0s 104us/step - loss: 0.4500 - acc: 0.8008 - val_loss: 0.4453 - val_acc: 0.8050\n",
      "Epoch 34/80\n",
      "2400/2400 [==============================] - 0s 94us/step - loss: 0.4469 - acc: 0.8033 - val_loss: 0.4423 - val_acc: 0.8058\n",
      "Epoch 35/80\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.4439 - acc: 0.8050 - val_loss: 0.4391 - val_acc: 0.8067\n",
      "Epoch 36/80\n",
      "2400/2400 [==============================] - 0s 90us/step - loss: 0.4410 - acc: 0.8046 - val_loss: 0.4364 - val_acc: 0.8100\n",
      "Epoch 37/80\n",
      "2400/2400 [==============================] - 0s 78us/step - loss: 0.4383 - acc: 0.8067 - val_loss: 0.4338 - val_acc: 0.8092\n",
      "Epoch 38/80\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 0.4359 - acc: 0.8075 - val_loss: 0.4313 - val_acc: 0.8117\n",
      "Epoch 39/80\n",
      "2400/2400 [==============================] - 0s 60us/step - loss: 0.4336 - acc: 0.8079 - val_loss: 0.4290 - val_acc: 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/80\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.4313 - acc: 0.8087 - val_loss: 0.4269 - val_acc: 0.8133\n",
      "Epoch 41/80\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.4290 - acc: 0.8083 - val_loss: 0.4251 - val_acc: 0.8142\n",
      "Epoch 42/80\n",
      "2400/2400 [==============================] - 0s 55us/step - loss: 0.4272 - acc: 0.8121 - val_loss: 0.4230 - val_acc: 0.8150\n",
      "Epoch 43/80\n",
      "2400/2400 [==============================] - 0s 54us/step - loss: 0.4252 - acc: 0.8121 - val_loss: 0.4213 - val_acc: 0.8150\n",
      "Epoch 44/80\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.4235 - acc: 0.8121 - val_loss: 0.4196 - val_acc: 0.8183\n",
      "Epoch 45/80\n",
      "2400/2400 [==============================] - 0s 56us/step - loss: 0.4217 - acc: 0.8125 - val_loss: 0.4181 - val_acc: 0.8175\n",
      "Epoch 46/80\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.4201 - acc: 0.8142 - val_loss: 0.4166 - val_acc: 0.8175\n",
      "Epoch 47/80\n",
      "2400/2400 [==============================] - 0s 57us/step - loss: 0.4185 - acc: 0.8125 - val_loss: 0.4152 - val_acc: 0.8175\n",
      "Epoch 48/80\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.4169 - acc: 0.8146 - val_loss: 0.4140 - val_acc: 0.8183\n",
      "Epoch 49/80\n",
      "2400/2400 [==============================] - 0s 78us/step - loss: 0.4157 - acc: 0.8129 - val_loss: 0.4127 - val_acc: 0.8183\n",
      "Epoch 50/80\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.4143 - acc: 0.8142 - val_loss: 0.4115 - val_acc: 0.8175\n",
      "Epoch 51/80\n",
      "2400/2400 [==============================] - 0s 54us/step - loss: 0.4128 - acc: 0.8129 - val_loss: 0.4104 - val_acc: 0.8183\n",
      "Epoch 52/80\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.4115 - acc: 0.8150 - val_loss: 0.4094 - val_acc: 0.8183\n",
      "Epoch 53/80\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 0.4104 - acc: 0.8133 - val_loss: 0.4084 - val_acc: 0.8175\n",
      "Epoch 54/80\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 0.4092 - acc: 0.8142 - val_loss: 0.4076 - val_acc: 0.8167\n",
      "Epoch 55/80\n",
      "2400/2400 [==============================] - 0s 62us/step - loss: 0.4079 - acc: 0.8129 - val_loss: 0.4066 - val_acc: 0.8175\n",
      "Epoch 56/80\n",
      "2400/2400 [==============================] - 0s 80us/step - loss: 0.4069 - acc: 0.8129 - val_loss: 0.4057 - val_acc: 0.8175\n",
      "Epoch 57/80\n",
      "2400/2400 [==============================] - 0s 55us/step - loss: 0.4056 - acc: 0.8133 - val_loss: 0.4049 - val_acc: 0.8183\n",
      "Epoch 58/80\n",
      "2400/2400 [==============================] - 0s 50us/step - loss: 0.4048 - acc: 0.8162 - val_loss: 0.4042 - val_acc: 0.8208\n",
      "Epoch 59/80\n",
      "2400/2400 [==============================] - 0s 54us/step - loss: 0.4039 - acc: 0.8158 - val_loss: 0.4035 - val_acc: 0.8225\n",
      "Epoch 60/80\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.4028 - acc: 0.8162 - val_loss: 0.4029 - val_acc: 0.8225\n",
      "Epoch 61/80\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.4018 - acc: 0.8175 - val_loss: 0.4023 - val_acc: 0.8250\n",
      "Epoch 62/80\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.4010 - acc: 0.8171 - val_loss: 0.4016 - val_acc: 0.8242\n",
      "Epoch 63/80\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.4001 - acc: 0.8175 - val_loss: 0.4011 - val_acc: 0.8258\n",
      "Epoch 64/80\n",
      "2400/2400 [==============================] - 0s 64us/step - loss: 0.3992 - acc: 0.8179 - val_loss: 0.4005 - val_acc: 0.8250\n",
      "Epoch 65/80\n",
      "2400/2400 [==============================] - 0s 82us/step - loss: 0.3984 - acc: 0.8196 - val_loss: 0.4000 - val_acc: 0.8250\n",
      "Epoch 66/80\n",
      "2400/2400 [==============================] - 0s 60us/step - loss: 0.3976 - acc: 0.8183 - val_loss: 0.3995 - val_acc: 0.8242\n",
      "Epoch 67/80\n",
      "2400/2400 [==============================] - 0s 58us/step - loss: 0.3969 - acc: 0.8200 - val_loss: 0.3991 - val_acc: 0.8250\n",
      "Epoch 68/80\n",
      "2400/2400 [==============================] - 0s 56us/step - loss: 0.3959 - acc: 0.8200 - val_loss: 0.3989 - val_acc: 0.8225\n",
      "Epoch 69/80\n",
      "2400/2400 [==============================] - 0s 56us/step - loss: 0.3956 - acc: 0.8204 - val_loss: 0.3981 - val_acc: 0.8242\n",
      "Epoch 70/80\n",
      "2400/2400 [==============================] - 0s 57us/step - loss: 0.3951 - acc: 0.8204 - val_loss: 0.3977 - val_acc: 0.8250\n",
      "Epoch 71/80\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 0.3938 - acc: 0.8204 - val_loss: 0.3974 - val_acc: 0.8283\n",
      "Epoch 72/80\n",
      "2400/2400 [==============================] - ETA: 0s - loss: 0.3953 - acc: 0.821 - 0s 84us/step - loss: 0.3935 - acc: 0.8208 - val_loss: 0.3968 - val_acc: 0.8283\n",
      "Epoch 73/80\n",
      "2400/2400 [==============================] - 0s 85us/step - loss: 0.3928 - acc: 0.8217 - val_loss: 0.3965 - val_acc: 0.8258\n",
      "Epoch 74/80\n",
      "2400/2400 [==============================] - 0s 65us/step - loss: 0.3922 - acc: 0.8221 - val_loss: 0.3962 - val_acc: 0.8275\n",
      "Epoch 75/80\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 0.3917 - acc: 0.8208 - val_loss: 0.3959 - val_acc: 0.8258\n",
      "Epoch 76/80\n",
      "2400/2400 [==============================] - 0s 57us/step - loss: 0.3912 - acc: 0.8200 - val_loss: 0.3956 - val_acc: 0.8258\n",
      "Epoch 77/80\n",
      "2400/2400 [==============================] - 0s 56us/step - loss: 0.3906 - acc: 0.8221 - val_loss: 0.3953 - val_acc: 0.8233\n",
      "Epoch 78/80\n",
      "2400/2400 [==============================] - 0s 56us/step - loss: 0.3899 - acc: 0.8258 - val_loss: 0.3950 - val_acc: 0.8267\n",
      "Epoch 79/80\n",
      "2400/2400 [==============================] - 0s 56us/step - loss: 0.3893 - acc: 0.8217 - val_loss: 0.3949 - val_acc: 0.8233\n",
      "Epoch 80/80\n",
      "2400/2400 [==============================] - 0s 61us/step - loss: 0.3890 - acc: 0.8229 - val_loss: 0.3946 - val_acc: 0.8250\n",
      "198/198 [==============================] - 0s 96us/step\n",
      "3200/3200 [==============================] - 0s 67us/step\n",
      "Train on 2400 samples, validate on 1200 samples\n",
      "Epoch 1/80\n",
      "2400/2400 [==============================] - 4s 2ms/step - loss: 0.6904 - acc: 0.5496 - val_loss: 0.6853 - val_acc: 0.5783\n",
      "Epoch 2/80\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.6808 - acc: 0.6075 - val_loss: 0.6762 - val_acc: 0.6425\n",
      "Epoch 3/80\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.6716 - acc: 0.6971 - val_loss: 0.6675 - val_acc: 0.7042\n",
      "Epoch 4/80\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.6627 - acc: 0.7250 - val_loss: 0.6588 - val_acc: 0.7383\n",
      "Epoch 5/80\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.6538 - acc: 0.7542 - val_loss: 0.6502 - val_acc: 0.7558\n",
      "Epoch 6/80\n",
      "2400/2400 [==============================] - 0s 50us/step - loss: 0.6448 - acc: 0.7612 - val_loss: 0.6415 - val_acc: 0.7617\n",
      "Epoch 7/80\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.6356 - acc: 0.7675 - val_loss: 0.6327 - val_acc: 0.7675\n",
      "Epoch 8/80\n",
      "2400/2400 [==============================] - 0s 49us/step - loss: 0.6263 - acc: 0.7642 - val_loss: 0.6238 - val_acc: 0.7683\n",
      "Epoch 9/80\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.6167 - acc: 0.7692 - val_loss: 0.6148 - val_acc: 0.7650\n",
      "Epoch 10/80\n",
      "2400/2400 [==============================] - 0s 58us/step - loss: 0.6071 - acc: 0.7658 - val_loss: 0.6057 - val_acc: 0.7650\n",
      "Epoch 11/80\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.5973 - acc: 0.7692 - val_loss: 0.5966 - val_acc: 0.7708\n",
      "Epoch 12/80\n",
      "2400/2400 [==============================] - 0s 55us/step - loss: 0.5875 - acc: 0.7692 - val_loss: 0.5874 - val_acc: 0.7742\n",
      "Epoch 13/80\n",
      "2400/2400 [==============================] - 0s 55us/step - loss: 0.5777 - acc: 0.7708 - val_loss: 0.5784 - val_acc: 0.7742\n",
      "Epoch 14/80\n",
      "2400/2400 [==============================] - 0s 55us/step - loss: 0.5680 - acc: 0.7717 - val_loss: 0.5695 - val_acc: 0.7742\n",
      "Epoch 15/80\n",
      "2400/2400 [==============================] - 0s 58us/step - loss: 0.5583 - acc: 0.7742 - val_loss: 0.5607 - val_acc: 0.7725\n",
      "Epoch 16/80\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.5488 - acc: 0.7754 - val_loss: 0.5523 - val_acc: 0.7733\n",
      "Epoch 17/80\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.5396 - acc: 0.7762 - val_loss: 0.5441 - val_acc: 0.7742\n",
      "Epoch 18/80\n",
      "2400/2400 [==============================] - 0s 50us/step - loss: 0.5306 - acc: 0.7762 - val_loss: 0.5362 - val_acc: 0.7767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/80\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.5221 - acc: 0.7796 - val_loss: 0.5287 - val_acc: 0.7775\n",
      "Epoch 20/80\n",
      "2400/2400 [==============================] - 0s 55us/step - loss: 0.5138 - acc: 0.7829 - val_loss: 0.5216 - val_acc: 0.7775\n",
      "Epoch 21/80\n",
      "2400/2400 [==============================] - 0s 61us/step - loss: 0.5060 - acc: 0.7825 - val_loss: 0.5149 - val_acc: 0.7808\n",
      "Epoch 22/80\n",
      "2400/2400 [==============================] - 0s 56us/step - loss: 0.4987 - acc: 0.7854 - val_loss: 0.5087 - val_acc: 0.7825\n",
      "Epoch 23/80\n",
      "2400/2400 [==============================] - 0s 57us/step - loss: 0.4918 - acc: 0.7850 - val_loss: 0.5028 - val_acc: 0.7808\n",
      "Epoch 24/80\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.4852 - acc: 0.7858 - val_loss: 0.4974 - val_acc: 0.7842\n",
      "Epoch 25/80\n",
      "2400/2400 [==============================] - 0s 58us/step - loss: 0.4791 - acc: 0.7875 - val_loss: 0.4923 - val_acc: 0.7858\n",
      "Epoch 26/80\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 0.4735 - acc: 0.7921 - val_loss: 0.4877 - val_acc: 0.7875\n",
      "Epoch 27/80\n",
      "2400/2400 [==============================] - 0s 54us/step - loss: 0.4682 - acc: 0.7921 - val_loss: 0.4834 - val_acc: 0.7883\n",
      "Epoch 28/80\n",
      "2400/2400 [==============================] - 0s 58us/step - loss: 0.4634 - acc: 0.7958 - val_loss: 0.4794 - val_acc: 0.7900\n",
      "Epoch 29/80\n",
      "2400/2400 [==============================] - 0s 50us/step - loss: 0.4588 - acc: 0.7988 - val_loss: 0.4757 - val_acc: 0.7900\n",
      "Epoch 30/80\n",
      "2400/2400 [==============================] - 0s 50us/step - loss: 0.4546 - acc: 0.7979 - val_loss: 0.4723 - val_acc: 0.7900\n",
      "Epoch 31/80\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.4507 - acc: 0.8013 - val_loss: 0.4692 - val_acc: 0.7908\n",
      "Epoch 32/80\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.4472 - acc: 0.8025 - val_loss: 0.4661 - val_acc: 0.7958\n",
      "Epoch 33/80\n",
      "2400/2400 [==============================] - 0s 57us/step - loss: 0.4438 - acc: 0.8033 - val_loss: 0.4635 - val_acc: 0.7967\n",
      "Epoch 34/80\n",
      "2400/2400 [==============================] - 0s 56us/step - loss: 0.4407 - acc: 0.8033 - val_loss: 0.4611 - val_acc: 0.7967\n",
      "Epoch 35/80\n",
      "2400/2400 [==============================] - 0s 63us/step - loss: 0.4377 - acc: 0.8042 - val_loss: 0.4588 - val_acc: 0.7983\n",
      "Epoch 36/80\n",
      "2400/2400 [==============================] - 0s 58us/step - loss: 0.4350 - acc: 0.8050 - val_loss: 0.4565 - val_acc: 0.8000\n",
      "Epoch 37/80\n",
      "2400/2400 [==============================] - 0s 56us/step - loss: 0.4324 - acc: 0.8054 - val_loss: 0.4541 - val_acc: 0.8000\n",
      "Epoch 38/80\n",
      "2400/2400 [==============================] - 0s 55us/step - loss: 0.4300 - acc: 0.8075 - val_loss: 0.4522 - val_acc: 0.8017\n",
      "Epoch 39/80\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.4278 - acc: 0.8075 - val_loss: 0.4505 - val_acc: 0.8000\n",
      "Epoch 40/80\n",
      "2400/2400 [==============================] - 0s 50us/step - loss: 0.4256 - acc: 0.8104 - val_loss: 0.4486 - val_acc: 0.8017\n",
      "Epoch 41/80\n",
      "2400/2400 [==============================] - 0s 59us/step - loss: 0.4236 - acc: 0.8104 - val_loss: 0.4470 - val_acc: 0.8033\n",
      "Epoch 42/80\n",
      "2400/2400 [==============================] - 0s 62us/step - loss: 0.4217 - acc: 0.8108 - val_loss: 0.4457 - val_acc: 0.8017\n",
      "Epoch 43/80\n",
      "2400/2400 [==============================] - 0s 55us/step - loss: 0.4201 - acc: 0.8138 - val_loss: 0.4441 - val_acc: 0.8017\n",
      "Epoch 44/80\n",
      "2400/2400 [==============================] - 0s 65us/step - loss: 0.4183 - acc: 0.8137 - val_loss: 0.4427 - val_acc: 0.8033\n",
      "Epoch 45/80\n",
      "2400/2400 [==============================] - 0s 55us/step - loss: 0.4168 - acc: 0.8142 - val_loss: 0.4415 - val_acc: 0.8033\n",
      "Epoch 46/80\n",
      "2400/2400 [==============================] - 0s 61us/step - loss: 0.4152 - acc: 0.8150 - val_loss: 0.4400 - val_acc: 0.8050\n",
      "Epoch 47/80\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.4139 - acc: 0.8138 - val_loss: 0.4386 - val_acc: 0.8058\n",
      "Epoch 48/80\n",
      "2400/2400 [==============================] - 0s 60us/step - loss: 0.4124 - acc: 0.8179 - val_loss: 0.4378 - val_acc: 0.8058\n",
      "Epoch 49/80\n",
      "2400/2400 [==============================] - 0s 61us/step - loss: 0.4112 - acc: 0.8175 - val_loss: 0.4367 - val_acc: 0.8058\n",
      "Epoch 50/80\n",
      "2400/2400 [==============================] - 0s 57us/step - loss: 0.4099 - acc: 0.8171 - val_loss: 0.4353 - val_acc: 0.8058\n",
      "Epoch 51/80\n",
      "2400/2400 [==============================] - 0s 60us/step - loss: 0.4088 - acc: 0.8183 - val_loss: 0.4343 - val_acc: 0.8075\n",
      "Epoch 52/80\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.4077 - acc: 0.8196 - val_loss: 0.4335 - val_acc: 0.8075\n",
      "Epoch 53/80\n",
      "2400/2400 [==============================] - 0s 63us/step - loss: 0.4065 - acc: 0.8188 - val_loss: 0.4328 - val_acc: 0.8067\n",
      "Epoch 54/80\n",
      "2400/2400 [==============================] - 0s 55us/step - loss: 0.4052 - acc: 0.8208 - val_loss: 0.4313 - val_acc: 0.8075\n",
      "Epoch 55/80\n",
      "2400/2400 [==============================] - 0s 56us/step - loss: 0.4046 - acc: 0.8200 - val_loss: 0.4305 - val_acc: 0.8075\n",
      "Epoch 56/80\n",
      "2400/2400 [==============================] - 0s 56us/step - loss: 0.4035 - acc: 0.8192 - val_loss: 0.4300 - val_acc: 0.8067\n",
      "Epoch 57/80\n",
      "2400/2400 [==============================] - 0s 56us/step - loss: 0.4026 - acc: 0.8196 - val_loss: 0.4293 - val_acc: 0.8058\n",
      "Epoch 58/80\n",
      "2400/2400 [==============================] - 0s 59us/step - loss: 0.4017 - acc: 0.8200 - val_loss: 0.4286 - val_acc: 0.8067\n",
      "Epoch 59/80\n",
      "2400/2400 [==============================] - 0s 57us/step - loss: 0.4009 - acc: 0.8196 - val_loss: 0.4276 - val_acc: 0.8083\n",
      "Epoch 60/80\n",
      "2400/2400 [==============================] - 0s 63us/step - loss: 0.4001 - acc: 0.8212 - val_loss: 0.4269 - val_acc: 0.8092\n",
      "Epoch 61/80\n",
      "2400/2400 [==============================] - 0s 56us/step - loss: 0.3992 - acc: 0.8221 - val_loss: 0.4258 - val_acc: 0.8108\n",
      "Epoch 62/80\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.3985 - acc: 0.8237 - val_loss: 0.4253 - val_acc: 0.8092\n",
      "Epoch 63/80\n",
      "2400/2400 [==============================] - 0s 65us/step - loss: 0.3978 - acc: 0.8221 - val_loss: 0.4247 - val_acc: 0.8083\n",
      "Epoch 64/80\n",
      "2400/2400 [==============================] - 0s 59us/step - loss: 0.3970 - acc: 0.8229 - val_loss: 0.4238 - val_acc: 0.8100\n",
      "Epoch 65/80\n",
      "2400/2400 [==============================] - 0s 65us/step - loss: 0.3965 - acc: 0.8237 - val_loss: 0.4235 - val_acc: 0.8083\n",
      "Epoch 66/80\n",
      "2400/2400 [==============================] - 0s 56us/step - loss: 0.3956 - acc: 0.8237 - val_loss: 0.4227 - val_acc: 0.8083\n",
      "Epoch 67/80\n",
      "2400/2400 [==============================] - 0s 64us/step - loss: 0.3950 - acc: 0.8237 - val_loss: 0.4224 - val_acc: 0.8083\n",
      "Epoch 68/80\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.3944 - acc: 0.8229 - val_loss: 0.4214 - val_acc: 0.8108\n",
      "Epoch 69/80\n",
      "2400/2400 [==============================] - 0s 57us/step - loss: 0.3938 - acc: 0.8242 - val_loss: 0.4209 - val_acc: 0.8100\n",
      "Epoch 70/80\n",
      "2400/2400 [==============================] - 0s 62us/step - loss: 0.3933 - acc: 0.8254 - val_loss: 0.4203 - val_acc: 0.8092\n",
      "Epoch 71/80\n",
      "2400/2400 [==============================] - 0s 55us/step - loss: 0.3928 - acc: 0.8242 - val_loss: 0.4198 - val_acc: 0.8092\n",
      "Epoch 72/80\n",
      "2400/2400 [==============================] - 0s 54us/step - loss: 0.3922 - acc: 0.8250 - val_loss: 0.4195 - val_acc: 0.8100\n",
      "Epoch 73/80\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.3917 - acc: 0.8246 - val_loss: 0.4191 - val_acc: 0.8100\n",
      "Epoch 74/80\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.3913 - acc: 0.8254 - val_loss: 0.4189 - val_acc: 0.8083\n",
      "Epoch 75/80\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.3906 - acc: 0.8246 - val_loss: 0.4180 - val_acc: 0.8108\n",
      "Epoch 76/80\n",
      "2400/2400 [==============================] - 0s 58us/step - loss: 0.3902 - acc: 0.8258 - val_loss: 0.4174 - val_acc: 0.8117\n",
      "Epoch 77/80\n",
      "2400/2400 [==============================] - 0s 55us/step - loss: 0.3898 - acc: 0.8258 - val_loss: 0.4175 - val_acc: 0.8108\n",
      "Epoch 78/80\n",
      "2400/2400 [==============================] - 0s 61us/step - loss: 0.3893 - acc: 0.8267 - val_loss: 0.4172 - val_acc: 0.8108\n",
      "Epoch 79/80\n",
      "2400/2400 [==============================] - 0s 56us/step - loss: 0.3887 - acc: 0.8267 - val_loss: 0.4161 - val_acc: 0.8133\n",
      "Epoch 80/80\n",
      "2400/2400 [==============================] - 0s 56us/step - loss: 0.3883 - acc: 0.8279 - val_loss: 0.4159 - val_acc: 0.8133\n",
      "198/198 [==============================] - 0s 130us/step\n",
      "3200/3200 [==============================] - 0s 71us/step\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 80\n",
    "dropout_size = 0.5\n",
    "n = 10\n",
    "\n",
    "nav_rec = []\n",
    "put_rec = []\n",
    "\n",
    "for i in np.arange(500, 600, 100):\n",
    "    X = np.vstack((X_4_full[0:1800, :], X_5_full[0:1800, :]))\n",
    "    pca = PCA(n_components=n)\n",
    "    pca.fit(X)\n",
    "    X_pca = pca.transform(X)\n",
    "    X_4_full_pca = pca.transform(X_4_full[1800:, :])\n",
    "    X_5_full_pca = pca.transform(X_5_full[1800:, :])\n",
    "    y = np.append(np.zeros(1800), np.ones(1800))\n",
    "    \n",
    "    nav = []\n",
    "    put = []\n",
    "    for train, test in kfold.split(X_pca, y):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(250, input_dim=n, activation='relu'))\n",
    "        Dropout(dropout_size)\n",
    "        model.add(Dense(100, activation='relu'))\n",
    "        Dropout(dropout_size/2)\n",
    "        model.add(Dense(40, activation='relu'))\n",
    "        Dropout(dropout_size/3)\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "        model.fit(X_pca[train], y[train], batch_size=batch_size, epochs=epochs, shuffle=True,\n",
    "                    validation_data=(X_pca[test], y[test]))\n",
    "        nav_scores = model.evaluate(X_4_full_pca, np.zeros(X_4_full_pca.shape[0]))\n",
    "        put_scores = model.evaluate(X_5_full_pca, np.ones(X_5_full_pca.shape[0]))\n",
    "        nav.append(round(nav_scores[1] * 100, 2))\n",
    "        put.append(round(put_scores[1] * 100, 2))\n",
    "    nav.append(n)\n",
    "    nav.append(i)\n",
    "    put.append(n)\n",
    "    put.append(i)\n",
    "    nav_rec.append(nav)\n",
    "    put_rec.append(put)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[81.310000000000002, 82.829999999999998, 80.299999999999997, 10, 500]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nav_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[93.120000000000005, 93.159999999999997, 93.879999999999995, 10, 500]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "put_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1332 samples, validate on 668 samples\n",
      "Epoch 1/80\n",
      "1332/1332 [==============================] - 15s 11ms/step - loss: 0.6824 - acc: 0.5893 - val_loss: 0.6791 - val_acc: 0.6213\n",
      "Epoch 2/80\n",
      "1332/1332 [==============================] - 0s 68us/step - loss: 0.6757 - acc: 0.6216 - val_loss: 0.6721 - val_acc: 0.6527\n",
      "Epoch 3/80\n",
      "1332/1332 [==============================] - 0s 64us/step - loss: 0.6693 - acc: 0.6607 - val_loss: 0.6655 - val_acc: 0.6826\n",
      "Epoch 4/80\n",
      "1332/1332 [==============================] - 0s 67us/step - loss: 0.6631 - acc: 0.6839 - val_loss: 0.6587 - val_acc: 0.7081\n",
      "Epoch 5/80\n",
      "1332/1332 [==============================] - 0s 64us/step - loss: 0.6566 - acc: 0.7020 - val_loss: 0.6520 - val_acc: 0.7260\n",
      "Epoch 6/80\n",
      "1332/1332 [==============================] - 0s 64us/step - loss: 0.6503 - acc: 0.7230 - val_loss: 0.6454 - val_acc: 0.7380\n",
      "Epoch 7/80\n",
      "1332/1332 [==============================] - 0s 71us/step - loss: 0.6440 - acc: 0.7335 - val_loss: 0.6388 - val_acc: 0.7455\n",
      "Epoch 8/80\n",
      "1332/1332 [==============================] - 0s 67us/step - loss: 0.6376 - acc: 0.7455 - val_loss: 0.6321 - val_acc: 0.7590\n",
      "Epoch 9/80\n",
      "1332/1332 [==============================] - 0s 70us/step - loss: 0.6312 - acc: 0.7500 - val_loss: 0.6254 - val_acc: 0.7650\n",
      "Epoch 10/80\n",
      "1332/1332 [==============================] - 0s 78us/step - loss: 0.6248 - acc: 0.7605 - val_loss: 0.6187 - val_acc: 0.7680\n",
      "Epoch 11/80\n",
      "1332/1332 [==============================] - 0s 73us/step - loss: 0.6184 - acc: 0.7673 - val_loss: 0.6120 - val_acc: 0.7695\n",
      "Epoch 12/80\n",
      "1332/1332 [==============================] - 0s 55us/step - loss: 0.6120 - acc: 0.7703 - val_loss: 0.6053 - val_acc: 0.7740\n",
      "Epoch 13/80\n",
      "1332/1332 [==============================] - 0s 64us/step - loss: 0.6056 - acc: 0.7755 - val_loss: 0.5986 - val_acc: 0.7754\n",
      "Epoch 14/80\n",
      "1332/1332 [==============================] - 0s 77us/step - loss: 0.5992 - acc: 0.7793 - val_loss: 0.5916 - val_acc: 0.7754\n",
      "Epoch 15/80\n",
      "1332/1332 [==============================] - 0s 71us/step - loss: 0.5926 - acc: 0.7785 - val_loss: 0.5848 - val_acc: 0.7754\n",
      "Epoch 16/80\n",
      "1332/1332 [==============================] - 0s 62us/step - loss: 0.5860 - acc: 0.7793 - val_loss: 0.5780 - val_acc: 0.7874\n",
      "Epoch 17/80\n",
      "1332/1332 [==============================] - 0s 81us/step - loss: 0.5795 - acc: 0.7868 - val_loss: 0.5711 - val_acc: 0.7874\n",
      "Epoch 18/80\n",
      "1332/1332 [==============================] - 0s 75us/step - loss: 0.5727 - acc: 0.7868 - val_loss: 0.5641 - val_acc: 0.7889\n",
      "Epoch 19/80\n",
      "1332/1332 [==============================] - 0s 69us/step - loss: 0.5658 - acc: 0.7913 - val_loss: 0.5570 - val_acc: 0.7904\n",
      "Epoch 20/80\n",
      "1332/1332 [==============================] - 0s 92us/step - loss: 0.5590 - acc: 0.7898 - val_loss: 0.5501 - val_acc: 0.7889\n",
      "Epoch 21/80\n",
      "1332/1332 [==============================] - 0s 88us/step - loss: 0.5522 - acc: 0.7958 - val_loss: 0.5432 - val_acc: 0.7889\n",
      "Epoch 22/80\n",
      "1332/1332 [==============================] - 0s 75us/step - loss: 0.5453 - acc: 0.7973 - val_loss: 0.5363 - val_acc: 0.7919\n",
      "Epoch 23/80\n",
      "1332/1332 [==============================] - 0s 74us/step - loss: 0.5386 - acc: 0.8018 - val_loss: 0.5295 - val_acc: 0.7934\n",
      "Epoch 24/80\n",
      "1332/1332 [==============================] - 0s 77us/step - loss: 0.5318 - acc: 0.8033 - val_loss: 0.5227 - val_acc: 0.7979\n",
      "Epoch 25/80\n",
      "1332/1332 [==============================] - 0s 94us/step - loss: 0.5251 - acc: 0.8041 - val_loss: 0.5161 - val_acc: 0.7994\n",
      "Epoch 26/80\n",
      "1332/1332 [==============================] - 0s 94us/step - loss: 0.5184 - acc: 0.8078 - val_loss: 0.5095 - val_acc: 0.7994\n",
      "Epoch 27/80\n",
      "1332/1332 [==============================] - 0s 100us/step - loss: 0.5118 - acc: 0.8116 - val_loss: 0.5029 - val_acc: 0.8024\n",
      "Epoch 28/80\n",
      "1332/1332 [==============================] - 0s 89us/step - loss: 0.5052 - acc: 0.8138 - val_loss: 0.4964 - val_acc: 0.8039\n",
      "Epoch 29/80\n",
      "1332/1332 [==============================] - 0s 120us/step - loss: 0.4987 - acc: 0.8131 - val_loss: 0.4902 - val_acc: 0.8069\n",
      "Epoch 30/80\n",
      "1332/1332 [==============================] - 0s 120us/step - loss: 0.4924 - acc: 0.8146 - val_loss: 0.4841 - val_acc: 0.8099\n",
      "Epoch 31/80\n",
      "1332/1332 [==============================] - 0s 81us/step - loss: 0.4860 - acc: 0.8146 - val_loss: 0.4782 - val_acc: 0.8114\n",
      "Epoch 32/80\n",
      "1332/1332 [==============================] - 0s 57us/step - loss: 0.4799 - acc: 0.8161 - val_loss: 0.4723 - val_acc: 0.8114\n",
      "Epoch 33/80\n",
      "1332/1332 [==============================] - 0s 96us/step - loss: 0.4739 - acc: 0.8168 - val_loss: 0.4669 - val_acc: 0.8144\n",
      "Epoch 34/80\n",
      "1332/1332 [==============================] - ETA: 0s - loss: 0.4633 - acc: 0.837 - 0s 97us/step - loss: 0.4681 - acc: 0.8183 - val_loss: 0.4614 - val_acc: 0.8159\n",
      "Epoch 35/80\n",
      "1332/1332 [==============================] - 0s 80us/step - loss: 0.4624 - acc: 0.8161 - val_loss: 0.4562 - val_acc: 0.8144\n",
      "Epoch 36/80\n",
      "1332/1332 [==============================] - 0s 80us/step - loss: 0.4568 - acc: 0.8183 - val_loss: 0.4511 - val_acc: 0.8159\n",
      "Epoch 37/80\n",
      "1332/1332 [==============================] - 0s 85us/step - loss: 0.4514 - acc: 0.8191 - val_loss: 0.4464 - val_acc: 0.8159\n",
      "Epoch 38/80\n",
      "1332/1332 [==============================] - 0s 98us/step - loss: 0.4460 - acc: 0.8191 - val_loss: 0.4418 - val_acc: 0.8144\n",
      "Epoch 39/80\n",
      "1332/1332 [==============================] - 0s 70us/step - loss: 0.4410 - acc: 0.8213 - val_loss: 0.4374 - val_acc: 0.8159\n",
      "Epoch 40/80\n",
      "1332/1332 [==============================] - 0s 87us/step - loss: 0.4362 - acc: 0.8243 - val_loss: 0.4332 - val_acc: 0.8174\n",
      "Epoch 41/80\n",
      "1332/1332 [==============================] - 0s 137us/step - loss: 0.4314 - acc: 0.8228 - val_loss: 0.4292 - val_acc: 0.8159\n",
      "Epoch 42/80\n",
      "1332/1332 [==============================] - 0s 69us/step - loss: 0.4268 - acc: 0.8258 - val_loss: 0.4254 - val_acc: 0.8144\n",
      "Epoch 43/80\n",
      "1332/1332 [==============================] - 0s 77us/step - loss: 0.4224 - acc: 0.8288 - val_loss: 0.4218 - val_acc: 0.8204\n",
      "Epoch 44/80\n",
      "1332/1332 [==============================] - 0s 92us/step - loss: 0.4180 - acc: 0.8318 - val_loss: 0.4184 - val_acc: 0.8189\n",
      "Epoch 45/80\n",
      "1332/1332 [==============================] - 0s 88us/step - loss: 0.4139 - acc: 0.8303 - val_loss: 0.4153 - val_acc: 0.8234\n",
      "Epoch 46/80\n",
      "1332/1332 [==============================] - 0s 61us/step - loss: 0.4104 - acc: 0.8348 - val_loss: 0.4121 - val_acc: 0.8249\n",
      "Epoch 47/80\n",
      "1332/1332 [==============================] - 0s 99us/step - loss: 0.4064 - acc: 0.8341 - val_loss: 0.4093 - val_acc: 0.8249\n",
      "Epoch 48/80\n",
      "1332/1332 [==============================] - 0s 93us/step - loss: 0.4027 - acc: 0.8378 - val_loss: 0.4064 - val_acc: 0.8249\n",
      "Epoch 49/80\n",
      "1332/1332 [==============================] - 0s 72us/step - loss: 0.3992 - acc: 0.8371 - val_loss: 0.4037 - val_acc: 0.8249\n",
      "Epoch 50/80\n",
      "1332/1332 [==============================] - 0s 81us/step - loss: 0.3958 - acc: 0.8378 - val_loss: 0.4014 - val_acc: 0.8263\n",
      "Epoch 51/80\n",
      "1332/1332 [==============================] - 0s 104us/step - loss: 0.3925 - acc: 0.8401 - val_loss: 0.3989 - val_acc: 0.8263\n",
      "Epoch 52/80\n",
      "1332/1332 [==============================] - 0s 75us/step - loss: 0.3894 - acc: 0.8401 - val_loss: 0.3966 - val_acc: 0.8234\n",
      "Epoch 53/80\n",
      "1332/1332 [==============================] - 0s 75us/step - loss: 0.3864 - acc: 0.8401 - val_loss: 0.3945 - val_acc: 0.8234\n",
      "Epoch 54/80\n",
      "1332/1332 [==============================] - 0s 89us/step - loss: 0.3835 - acc: 0.8401 - val_loss: 0.3925 - val_acc: 0.8263\n",
      "Epoch 55/80\n",
      "1332/1332 [==============================] - 0s 73us/step - loss: 0.3808 - acc: 0.8416 - val_loss: 0.3906 - val_acc: 0.8219\n",
      "Epoch 56/80\n",
      "1332/1332 [==============================] - 0s 96us/step - loss: 0.3782 - acc: 0.8401 - val_loss: 0.3889 - val_acc: 0.8234\n",
      "Epoch 57/80\n",
      "1332/1332 [==============================] - 0s 69us/step - loss: 0.3757 - acc: 0.8423 - val_loss: 0.3873 - val_acc: 0.8263\n",
      "Epoch 58/80\n",
      "1332/1332 [==============================] - 0s 78us/step - loss: 0.3730 - acc: 0.8416 - val_loss: 0.3857 - val_acc: 0.8278\n",
      "Epoch 59/80\n",
      "1332/1332 [==============================] - 0s 66us/step - loss: 0.3707 - acc: 0.8468 - val_loss: 0.3842 - val_acc: 0.8278\n",
      "Epoch 60/80\n",
      "1332/1332 [==============================] - 0s 89us/step - loss: 0.3685 - acc: 0.8461 - val_loss: 0.3829 - val_acc: 0.8308\n",
      "Epoch 61/80\n",
      "1332/1332 [==============================] - 0s 81us/step - loss: 0.3663 - acc: 0.8476 - val_loss: 0.3816 - val_acc: 0.8293\n",
      "Epoch 62/80\n",
      "1332/1332 [==============================] - 0s 69us/step - loss: 0.3643 - acc: 0.8483 - val_loss: 0.3805 - val_acc: 0.8323\n",
      "Epoch 63/80\n",
      "1332/1332 [==============================] - 0s 66us/step - loss: 0.3626 - acc: 0.8491 - val_loss: 0.3793 - val_acc: 0.8323\n",
      "Epoch 64/80\n",
      "1332/1332 [==============================] - 0s 76us/step - loss: 0.3604 - acc: 0.8521 - val_loss: 0.3784 - val_acc: 0.8323\n",
      "Epoch 65/80\n",
      "1332/1332 [==============================] - 0s 85us/step - loss: 0.3586 - acc: 0.8514 - val_loss: 0.3774 - val_acc: 0.8323\n",
      "Epoch 66/80\n",
      "1332/1332 [==============================] - 0s 75us/step - loss: 0.3569 - acc: 0.8521 - val_loss: 0.3766 - val_acc: 0.8293\n",
      "Epoch 67/80\n",
      "1332/1332 [==============================] - 0s 67us/step - loss: 0.3554 - acc: 0.8498 - val_loss: 0.3757 - val_acc: 0.8293\n",
      "Epoch 68/80\n",
      "1332/1332 [==============================] - 0s 84us/step - loss: 0.3536 - acc: 0.8529 - val_loss: 0.3750 - val_acc: 0.8338\n",
      "Epoch 69/80\n",
      "1332/1332 [==============================] - 0s 73us/step - loss: 0.3522 - acc: 0.8536 - val_loss: 0.3743 - val_acc: 0.8323\n",
      "Epoch 70/80\n",
      "1332/1332 [==============================] - 0s 75us/step - loss: 0.3509 - acc: 0.8506 - val_loss: 0.3736 - val_acc: 0.8323\n",
      "Epoch 71/80\n",
      "1332/1332 [==============================] - 0s 68us/step - loss: 0.3496 - acc: 0.8521 - val_loss: 0.3730 - val_acc: 0.8353\n",
      "Epoch 72/80\n",
      "1332/1332 [==============================] - 0s 72us/step - loss: 0.3482 - acc: 0.8551 - val_loss: 0.3724 - val_acc: 0.8368\n",
      "Epoch 73/80\n",
      "1332/1332 [==============================] - 0s 61us/step - loss: 0.3470 - acc: 0.8544 - val_loss: 0.3717 - val_acc: 0.8353\n",
      "Epoch 74/80\n",
      "1332/1332 [==============================] - 0s 90us/step - loss: 0.3456 - acc: 0.8574 - val_loss: 0.3712 - val_acc: 0.8368\n",
      "Epoch 75/80\n",
      "1332/1332 [==============================] - 0s 77us/step - loss: 0.3444 - acc: 0.8581 - val_loss: 0.3707 - val_acc: 0.8368\n",
      "Epoch 76/80\n",
      "1332/1332 [==============================] - 0s 68us/step - loss: 0.3433 - acc: 0.8566 - val_loss: 0.3701 - val_acc: 0.8368\n",
      "Epoch 77/80\n",
      "1332/1332 [==============================] - 0s 88us/step - loss: 0.3421 - acc: 0.8604 - val_loss: 0.3698 - val_acc: 0.8383\n",
      "Epoch 78/80\n",
      "1332/1332 [==============================] - 0s 81us/step - loss: 0.3409 - acc: 0.8596 - val_loss: 0.3693 - val_acc: 0.8383\n",
      "Epoch 79/80\n",
      "1332/1332 [==============================] - 0s 84us/step - loss: 0.3400 - acc: 0.8589 - val_loss: 0.3689 - val_acc: 0.8398\n",
      "Epoch 80/80\n",
      "1332/1332 [==============================] - 0s 66us/step - loss: 0.3390 - acc: 0.8596 - val_loss: 0.3686 - val_acc: 0.8398\n",
      "1998/1998 [==============================] - 0s 78us/step\n",
      "5000/5000 [==============================] - 0s 74us/step\n",
      "Train on 1334 samples, validate on 666 samples\n",
      "Epoch 1/80\n",
      "1334/1334 [==============================] - 1s 435us/step - loss: 0.6909 - acc: 0.5247 - val_loss: 0.6875 - val_acc: 0.6036\n",
      "Epoch 2/80\n",
      "1334/1334 [==============================] - 0s 70us/step - loss: 0.6866 - acc: 0.5832 - val_loss: 0.6840 - val_acc: 0.6396\n",
      "Epoch 3/80\n",
      "1334/1334 [==============================] - 0s 71us/step - loss: 0.6825 - acc: 0.6529 - val_loss: 0.6805 - val_acc: 0.6712\n",
      "Epoch 4/80\n",
      "1334/1334 [==============================] - 0s 86us/step - loss: 0.6783 - acc: 0.6957 - val_loss: 0.6769 - val_acc: 0.7072\n",
      "Epoch 5/80\n",
      "1334/1334 [==============================] - 0s 79us/step - loss: 0.6740 - acc: 0.7369 - val_loss: 0.6734 - val_acc: 0.7297\n",
      "Epoch 6/80\n",
      "1334/1334 [==============================] - 0s 92us/step - loss: 0.6698 - acc: 0.7571 - val_loss: 0.6697 - val_acc: 0.7523\n",
      "Epoch 7/80\n",
      "1334/1334 [==============================] - 0s 67us/step - loss: 0.6654 - acc: 0.7714 - val_loss: 0.6659 - val_acc: 0.7553\n",
      "Epoch 8/80\n",
      "1334/1334 [==============================] - 0s 69us/step - loss: 0.6610 - acc: 0.7759 - val_loss: 0.6621 - val_acc: 0.7688\n",
      "Epoch 9/80\n",
      "1334/1334 [==============================] - 0s 64us/step - loss: 0.6566 - acc: 0.7804 - val_loss: 0.6581 - val_acc: 0.7688\n",
      "Epoch 10/80\n",
      "1334/1334 [==============================] - 0s 68us/step - loss: 0.6520 - acc: 0.7811 - val_loss: 0.6540 - val_acc: 0.7763\n",
      "Epoch 11/80\n",
      "1334/1334 [==============================] - 0s 61us/step - loss: 0.6472 - acc: 0.7819 - val_loss: 0.6495 - val_acc: 0.7808\n",
      "Epoch 12/80\n",
      "1334/1334 [==============================] - 0s 89us/step - loss: 0.6420 - acc: 0.7864 - val_loss: 0.6450 - val_acc: 0.7838\n",
      "Epoch 13/80\n",
      "1334/1334 [==============================] - 0s 78us/step - loss: 0.6368 - acc: 0.7871 - val_loss: 0.6404 - val_acc: 0.7868\n",
      "Epoch 14/80\n",
      "1334/1334 [==============================] - 0s 69us/step - loss: 0.6316 - acc: 0.7924 - val_loss: 0.6356 - val_acc: 0.7928\n",
      "Epoch 15/80\n",
      "1334/1334 [==============================] - 0s 68us/step - loss: 0.6261 - acc: 0.7961 - val_loss: 0.6305 - val_acc: 0.7928\n",
      "Epoch 16/80\n",
      "1334/1334 [==============================] - 0s 83us/step - loss: 0.6205 - acc: 0.7984 - val_loss: 0.6256 - val_acc: 0.7898\n",
      "Epoch 17/80\n",
      "1334/1334 [==============================] - 0s 89us/step - loss: 0.6147 - acc: 0.7976 - val_loss: 0.6203 - val_acc: 0.7928\n",
      "Epoch 18/80\n",
      "1334/1334 [==============================] - 0s 75us/step - loss: 0.6087 - acc: 0.7931 - val_loss: 0.6150 - val_acc: 0.7928\n",
      "Epoch 19/80\n",
      "1334/1334 [==============================] - 0s 64us/step - loss: 0.6027 - acc: 0.7954 - val_loss: 0.6096 - val_acc: 0.7943\n",
      "Epoch 20/80\n",
      "1334/1334 [==============================] - 0s 51us/step - loss: 0.5967 - acc: 0.7984 - val_loss: 0.6041 - val_acc: 0.7943\n",
      "Epoch 21/80\n",
      "1334/1334 [==============================] - 0s 59us/step - loss: 0.5905 - acc: 0.7954 - val_loss: 0.5985 - val_acc: 0.7943\n",
      "Epoch 22/80\n",
      "1334/1334 [==============================] - 0s 81us/step - loss: 0.5843 - acc: 0.8006 - val_loss: 0.5930 - val_acc: 0.7943\n",
      "Epoch 23/80\n",
      "1334/1334 [==============================] - 0s 93us/step - loss: 0.5781 - acc: 0.8013 - val_loss: 0.5872 - val_acc: 0.7928\n",
      "Epoch 24/80\n",
      "1334/1334 [==============================] - 0s 80us/step - loss: 0.5715 - acc: 0.8021 - val_loss: 0.5814 - val_acc: 0.7943\n",
      "Epoch 25/80\n",
      "1334/1334 [==============================] - 0s 71us/step - loss: 0.5652 - acc: 0.8028 - val_loss: 0.5756 - val_acc: 0.7958\n",
      "Epoch 26/80\n",
      "1334/1334 [==============================] - 0s 75us/step - loss: 0.5588 - acc: 0.8051 - val_loss: 0.5697 - val_acc: 0.7958\n",
      "Epoch 27/80\n",
      "1334/1334 [==============================] - 0s 79us/step - loss: 0.5523 - acc: 0.8058 - val_loss: 0.5639 - val_acc: 0.7958\n",
      "Epoch 28/80\n",
      "1334/1334 [==============================] - 0s 80us/step - loss: 0.5459 - acc: 0.8073 - val_loss: 0.5578 - val_acc: 0.7958\n",
      "Epoch 29/80\n",
      "1334/1334 [==============================] - ETA: 0s - loss: 0.5367 - acc: 0.811 - 0s 70us/step - loss: 0.5393 - acc: 0.8058 - val_loss: 0.5520 - val_acc: 0.7973\n",
      "Epoch 30/80\n",
      "1334/1334 [==============================] - 0s 94us/step - loss: 0.5330 - acc: 0.8066 - val_loss: 0.5460 - val_acc: 0.7973\n",
      "Epoch 31/80\n",
      "1334/1334 [==============================] - 0s 88us/step - loss: 0.5265 - acc: 0.8111 - val_loss: 0.5401 - val_acc: 0.7973\n",
      "Epoch 32/80\n",
      "1334/1334 [==============================] - 0s 61us/step - loss: 0.5202 - acc: 0.8118 - val_loss: 0.5342 - val_acc: 0.7973\n",
      "Epoch 33/80\n",
      "1334/1334 [==============================] - 0s 90us/step - loss: 0.5139 - acc: 0.8126 - val_loss: 0.5281 - val_acc: 0.7988\n",
      "Epoch 34/80\n",
      "1334/1334 [==============================] - 0s 88us/step - loss: 0.5074 - acc: 0.8126 - val_loss: 0.5221 - val_acc: 0.8003\n",
      "Epoch 35/80\n",
      "1334/1334 [==============================] - 0s 88us/step - loss: 0.5012 - acc: 0.8156 - val_loss: 0.5163 - val_acc: 0.8018\n",
      "Epoch 36/80\n",
      "1334/1334 [==============================] - 0s 62us/step - loss: 0.4952 - acc: 0.8133 - val_loss: 0.5106 - val_acc: 0.8033\n",
      "Epoch 37/80\n",
      "1334/1334 [==============================] - 0s 92us/step - loss: 0.4893 - acc: 0.8141 - val_loss: 0.5051 - val_acc: 0.8018\n",
      "Epoch 38/80\n",
      "1334/1334 [==============================] - 0s 82us/step - loss: 0.4835 - acc: 0.8156 - val_loss: 0.4995 - val_acc: 0.8048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/80\n",
      "1334/1334 [==============================] - 0s 58us/step - loss: 0.4779 - acc: 0.8163 - val_loss: 0.4940 - val_acc: 0.8048\n",
      "Epoch 40/80\n",
      "1334/1334 [==============================] - 0s 97us/step - loss: 0.4724 - acc: 0.8171 - val_loss: 0.4888 - val_acc: 0.8048\n",
      "Epoch 41/80\n",
      "1334/1334 [==============================] - 0s 85us/step - loss: 0.4672 - acc: 0.8178 - val_loss: 0.4836 - val_acc: 0.8048\n",
      "Epoch 42/80\n",
      "1334/1334 [==============================] - 0s 77us/step - loss: 0.4620 - acc: 0.8193 - val_loss: 0.4786 - val_acc: 0.8048\n",
      "Epoch 43/80\n",
      "1334/1334 [==============================] - 0s 82us/step - loss: 0.4569 - acc: 0.8186 - val_loss: 0.4738 - val_acc: 0.8048\n",
      "Epoch 44/80\n",
      "1334/1334 [==============================] - 0s 80us/step - loss: 0.4521 - acc: 0.8186 - val_loss: 0.4690 - val_acc: 0.8063\n",
      "Epoch 45/80\n",
      "1334/1334 [==============================] - 0s 70us/step - loss: 0.4475 - acc: 0.8223 - val_loss: 0.4643 - val_acc: 0.8063\n",
      "Epoch 46/80\n",
      "1334/1334 [==============================] - 0s 74us/step - loss: 0.4429 - acc: 0.8246 - val_loss: 0.4598 - val_acc: 0.8063\n",
      "Epoch 47/80\n",
      "1334/1334 [==============================] - 0s 78us/step - loss: 0.4386 - acc: 0.8261 - val_loss: 0.4555 - val_acc: 0.8078\n",
      "Epoch 48/80\n",
      "1334/1334 [==============================] - 0s 114us/step - loss: 0.4344 - acc: 0.8268 - val_loss: 0.4513 - val_acc: 0.8078\n",
      "Epoch 49/80\n",
      "1334/1334 [==============================] - 0s 88us/step - loss: 0.4303 - acc: 0.8283 - val_loss: 0.4474 - val_acc: 0.8078\n",
      "Epoch 50/80\n",
      "1334/1334 [==============================] - 0s 67us/step - loss: 0.4265 - acc: 0.8321 - val_loss: 0.4437 - val_acc: 0.8093\n",
      "Epoch 51/80\n",
      "1334/1334 [==============================] - 0s 45us/step - loss: 0.4229 - acc: 0.8313 - val_loss: 0.4397 - val_acc: 0.8153\n",
      "Epoch 52/80\n",
      "1334/1334 [==============================] - 0s 75us/step - loss: 0.4193 - acc: 0.8321 - val_loss: 0.4364 - val_acc: 0.8123\n",
      "Epoch 53/80\n",
      "1334/1334 [==============================] - 0s 109us/step - loss: 0.4158 - acc: 0.8328 - val_loss: 0.4330 - val_acc: 0.8123\n",
      "Epoch 54/80\n",
      "1334/1334 [==============================] - 0s 74us/step - loss: 0.4126 - acc: 0.8321 - val_loss: 0.4294 - val_acc: 0.8153\n",
      "Epoch 55/80\n",
      "1334/1334 [==============================] - 0s 70us/step - loss: 0.4094 - acc: 0.8351 - val_loss: 0.4262 - val_acc: 0.8153\n",
      "Epoch 56/80\n",
      "1334/1334 [==============================] - 0s 96us/step - loss: 0.4064 - acc: 0.8351 - val_loss: 0.4228 - val_acc: 0.8213\n",
      "Epoch 57/80\n",
      "1334/1334 [==============================] - 0s 74us/step - loss: 0.4035 - acc: 0.8343 - val_loss: 0.4199 - val_acc: 0.8183\n",
      "Epoch 58/80\n",
      "1334/1334 [==============================] - 0s 77us/step - loss: 0.4007 - acc: 0.8351 - val_loss: 0.4168 - val_acc: 0.8198\n",
      "Epoch 59/80\n",
      "1334/1334 [==============================] - 0s 86us/step - loss: 0.3980 - acc: 0.8343 - val_loss: 0.4137 - val_acc: 0.8213\n",
      "Epoch 60/80\n",
      "1334/1334 [==============================] - 0s 81us/step - loss: 0.3954 - acc: 0.8351 - val_loss: 0.4111 - val_acc: 0.8228\n",
      "Epoch 61/80\n",
      "1334/1334 [==============================] - 0s 70us/step - loss: 0.3930 - acc: 0.8388 - val_loss: 0.4089 - val_acc: 0.8258\n",
      "Epoch 62/80\n",
      "1334/1334 [==============================] - 0s 73us/step - loss: 0.3906 - acc: 0.8366 - val_loss: 0.4061 - val_acc: 0.8243\n",
      "Epoch 63/80\n",
      "1334/1334 [==============================] - 0s 79us/step - loss: 0.3884 - acc: 0.8381 - val_loss: 0.4034 - val_acc: 0.8258\n",
      "Epoch 64/80\n",
      "1334/1334 [==============================] - 0s 74us/step - loss: 0.3860 - acc: 0.8388 - val_loss: 0.4011 - val_acc: 0.8288\n",
      "Epoch 65/80\n",
      "1334/1334 [==============================] - 0s 72us/step - loss: 0.3839 - acc: 0.8411 - val_loss: 0.3989 - val_acc: 0.8288\n",
      "Epoch 66/80\n",
      "1334/1334 [==============================] - 0s 83us/step - loss: 0.3821 - acc: 0.8396 - val_loss: 0.3966 - val_acc: 0.8288\n",
      "Epoch 67/80\n",
      "1334/1334 [==============================] - 0s 77us/step - loss: 0.3801 - acc: 0.8418 - val_loss: 0.3945 - val_acc: 0.8288\n",
      "Epoch 68/80\n",
      "1334/1334 [==============================] - 0s 89us/step - loss: 0.3782 - acc: 0.8426 - val_loss: 0.3923 - val_acc: 0.8273\n",
      "Epoch 69/80\n",
      "1334/1334 [==============================] - 0s 99us/step - loss: 0.3764 - acc: 0.8426 - val_loss: 0.3905 - val_acc: 0.8273\n",
      "Epoch 70/80\n",
      "1334/1334 [==============================] - 0s 83us/step - loss: 0.3746 - acc: 0.8441 - val_loss: 0.3889 - val_acc: 0.8273\n",
      "Epoch 71/80\n",
      "1334/1334 [==============================] - 0s 107us/step - loss: 0.3731 - acc: 0.8448 - val_loss: 0.3870 - val_acc: 0.8273\n",
      "Epoch 72/80\n",
      "1334/1334 [==============================] - 0s 74us/step - loss: 0.3715 - acc: 0.8456 - val_loss: 0.3853 - val_acc: 0.8273\n",
      "Epoch 73/80\n",
      "1334/1334 [==============================] - 0s 66us/step - loss: 0.3701 - acc: 0.8471 - val_loss: 0.3838 - val_acc: 0.8333\n",
      "Epoch 74/80\n",
      "1334/1334 [==============================] - 0s 90us/step - loss: 0.3685 - acc: 0.8441 - val_loss: 0.3815 - val_acc: 0.8348\n",
      "Epoch 75/80\n",
      "1334/1334 [==============================] - 0s 76us/step - loss: 0.3673 - acc: 0.8516 - val_loss: 0.3801 - val_acc: 0.8348\n",
      "Epoch 76/80\n",
      "1334/1334 [==============================] - 0s 74us/step - loss: 0.3658 - acc: 0.8501 - val_loss: 0.3785 - val_acc: 0.8348\n",
      "Epoch 77/80\n",
      "1334/1334 [==============================] - 0s 81us/step - loss: 0.3646 - acc: 0.8508 - val_loss: 0.3769 - val_acc: 0.8408\n",
      "Epoch 78/80\n",
      "1334/1334 [==============================] - 0s 76us/step - loss: 0.3633 - acc: 0.8508 - val_loss: 0.3757 - val_acc: 0.8408\n",
      "Epoch 79/80\n",
      "1334/1334 [==============================] - 0s 61us/step - loss: 0.3619 - acc: 0.8516 - val_loss: 0.3745 - val_acc: 0.8408\n",
      "Epoch 80/80\n",
      "1334/1334 [==============================] - 0s 98us/step - loss: 0.3608 - acc: 0.8523 - val_loss: 0.3728 - val_acc: 0.8438\n",
      "1998/1998 [==============================] - 0s 76us/step\n",
      "5000/5000 [==============================] - 0s 77us/step\n",
      "Train on 1334 samples, validate on 666 samples\n",
      "Epoch 1/80\n",
      "1334/1334 [==============================] - 1s 445us/step - loss: 0.6832 - acc: 0.5915 - val_loss: 0.6788 - val_acc: 0.6081\n",
      "Epoch 2/80\n",
      "1334/1334 [==============================] - 0s 59us/step - loss: 0.6786 - acc: 0.6282 - val_loss: 0.6742 - val_acc: 0.6547\n",
      "Epoch 3/80\n",
      "1334/1334 [==============================] - 0s 94us/step - loss: 0.6742 - acc: 0.6694 - val_loss: 0.6693 - val_acc: 0.7087\n",
      "Epoch 4/80\n",
      "1334/1334 [==============================] - 0s 106us/step - loss: 0.6695 - acc: 0.7031 - val_loss: 0.6646 - val_acc: 0.7327\n",
      "Epoch 5/80\n",
      "1334/1334 [==============================] - 0s 63us/step - loss: 0.6650 - acc: 0.7376 - val_loss: 0.6599 - val_acc: 0.7658\n",
      "Epoch 6/80\n",
      "1334/1334 [==============================] - 0s 93us/step - loss: 0.6604 - acc: 0.7609 - val_loss: 0.6551 - val_acc: 0.7703\n",
      "Epoch 7/80\n",
      "1334/1334 [==============================] - 0s 71us/step - loss: 0.6557 - acc: 0.7766 - val_loss: 0.6502 - val_acc: 0.7808\n",
      "Epoch 8/80\n",
      "1334/1334 [==============================] - 0s 83us/step - loss: 0.6509 - acc: 0.7939 - val_loss: 0.6453 - val_acc: 0.8048\n",
      "Epoch 9/80\n",
      "1334/1334 [==============================] - 0s 82us/step - loss: 0.6461 - acc: 0.8073 - val_loss: 0.6404 - val_acc: 0.8108\n",
      "Epoch 10/80\n",
      "1334/1334 [==============================] - 0s 97us/step - loss: 0.6412 - acc: 0.8043 - val_loss: 0.6353 - val_acc: 0.8108\n",
      "Epoch 11/80\n",
      "1334/1334 [==============================] - 0s 92us/step - loss: 0.6361 - acc: 0.8096 - val_loss: 0.6302 - val_acc: 0.8138\n",
      "Epoch 12/80\n",
      "1334/1334 [==============================] - 0s 97us/step - loss: 0.6310 - acc: 0.8096 - val_loss: 0.6248 - val_acc: 0.8138\n",
      "Epoch 13/80\n",
      "1334/1334 [==============================] - 0s 87us/step - loss: 0.6256 - acc: 0.8141 - val_loss: 0.6193 - val_acc: 0.8138\n",
      "Epoch 14/80\n",
      "1334/1334 [==============================] - 0s 99us/step - loss: 0.6202 - acc: 0.8118 - val_loss: 0.6136 - val_acc: 0.8153\n",
      "Epoch 15/80\n",
      "1334/1334 [==============================] - 0s 76us/step - loss: 0.6145 - acc: 0.8141 - val_loss: 0.6078 - val_acc: 0.8108\n",
      "Epoch 16/80\n",
      "1334/1334 [==============================] - 0s 98us/step - loss: 0.6088 - acc: 0.8141 - val_loss: 0.6020 - val_acc: 0.8108\n",
      "Epoch 17/80\n",
      "1334/1334 [==============================] - 0s 86us/step - loss: 0.6029 - acc: 0.8141 - val_loss: 0.5960 - val_acc: 0.8123\n",
      "Epoch 18/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1334/1334 [==============================] - 0s 80us/step - loss: 0.5970 - acc: 0.8133 - val_loss: 0.5897 - val_acc: 0.8123\n",
      "Epoch 19/80\n",
      "1334/1334 [==============================] - 0s 88us/step - loss: 0.5908 - acc: 0.8141 - val_loss: 0.5833 - val_acc: 0.8153\n",
      "Epoch 20/80\n",
      "1334/1334 [==============================] - 0s 79us/step - loss: 0.5845 - acc: 0.8111 - val_loss: 0.5770 - val_acc: 0.8168\n",
      "Epoch 21/80\n",
      "1334/1334 [==============================] - 0s 94us/step - loss: 0.5782 - acc: 0.8126 - val_loss: 0.5707 - val_acc: 0.8183\n",
      "Epoch 22/80\n",
      "1334/1334 [==============================] - 0s 95us/step - loss: 0.5719 - acc: 0.8111 - val_loss: 0.5642 - val_acc: 0.8213\n",
      "Epoch 23/80\n",
      "1334/1334 [==============================] - 0s 79us/step - loss: 0.5656 - acc: 0.8096 - val_loss: 0.5576 - val_acc: 0.8198\n",
      "Epoch 24/80\n",
      "1334/1334 [==============================] - 0s 85us/step - loss: 0.5590 - acc: 0.8096 - val_loss: 0.5510 - val_acc: 0.8183\n",
      "Epoch 25/80\n",
      "1334/1334 [==============================] - 0s 73us/step - loss: 0.5526 - acc: 0.8088 - val_loss: 0.5444 - val_acc: 0.8183\n",
      "Epoch 26/80\n",
      "1334/1334 [==============================] - 0s 81us/step - loss: 0.5459 - acc: 0.8096 - val_loss: 0.5377 - val_acc: 0.8168\n",
      "Epoch 27/80\n",
      "1334/1334 [==============================] - 0s 76us/step - loss: 0.5394 - acc: 0.8088 - val_loss: 0.5310 - val_acc: 0.8183\n",
      "Epoch 28/80\n",
      "1334/1334 [==============================] - 0s 74us/step - loss: 0.5328 - acc: 0.8088 - val_loss: 0.5244 - val_acc: 0.8183\n",
      "Epoch 29/80\n",
      "1334/1334 [==============================] - 0s 84us/step - loss: 0.5264 - acc: 0.8088 - val_loss: 0.5180 - val_acc: 0.8153\n",
      "Epoch 30/80\n",
      "1334/1334 [==============================] - 0s 83us/step - loss: 0.5201 - acc: 0.8088 - val_loss: 0.5117 - val_acc: 0.8228\n",
      "Epoch 31/80\n",
      "1334/1334 [==============================] - 0s 71us/step - loss: 0.5138 - acc: 0.8088 - val_loss: 0.5054 - val_acc: 0.8213\n",
      "Epoch 32/80\n",
      "1334/1334 [==============================] - 0s 81us/step - loss: 0.5076 - acc: 0.8111 - val_loss: 0.4993 - val_acc: 0.8243\n",
      "Epoch 33/80\n",
      "1334/1334 [==============================] - 0s 82us/step - loss: 0.5016 - acc: 0.8096 - val_loss: 0.4933 - val_acc: 0.8258\n",
      "Epoch 34/80\n",
      "1334/1334 [==============================] - 0s 69us/step - loss: 0.4958 - acc: 0.8118 - val_loss: 0.4873 - val_acc: 0.8258\n",
      "Epoch 35/80\n",
      "1334/1334 [==============================] - 0s 84us/step - loss: 0.4900 - acc: 0.8133 - val_loss: 0.4817 - val_acc: 0.8333\n",
      "Epoch 36/80\n",
      "1334/1334 [==============================] - 0s 92us/step - loss: 0.4844 - acc: 0.8141 - val_loss: 0.4761 - val_acc: 0.8363\n",
      "Epoch 37/80\n",
      "1334/1334 [==============================] - 0s 75us/step - loss: 0.4789 - acc: 0.8148 - val_loss: 0.4705 - val_acc: 0.8393\n",
      "Epoch 38/80\n",
      "1334/1334 [==============================] - 0s 85us/step - loss: 0.4734 - acc: 0.8163 - val_loss: 0.4651 - val_acc: 0.8378\n",
      "Epoch 39/80\n",
      "1334/1334 [==============================] - 0s 100us/step - loss: 0.4681 - acc: 0.8171 - val_loss: 0.4599 - val_acc: 0.8393\n",
      "Epoch 40/80\n",
      "1334/1334 [==============================] - 0s 74us/step - loss: 0.4630 - acc: 0.8186 - val_loss: 0.4548 - val_acc: 0.8393\n",
      "Epoch 41/80\n",
      "1334/1334 [==============================] - 0s 96us/step - loss: 0.4581 - acc: 0.8186 - val_loss: 0.4499 - val_acc: 0.8393\n",
      "Epoch 42/80\n",
      "1334/1334 [==============================] - 0s 83us/step - loss: 0.4533 - acc: 0.8193 - val_loss: 0.4452 - val_acc: 0.8393\n",
      "Epoch 43/80\n",
      "1334/1334 [==============================] - 0s 70us/step - loss: 0.4488 - acc: 0.8186 - val_loss: 0.4406 - val_acc: 0.8393\n",
      "Epoch 44/80\n",
      "1334/1334 [==============================] - 0s 93us/step - loss: 0.4443 - acc: 0.8193 - val_loss: 0.4363 - val_acc: 0.8393\n",
      "Epoch 45/80\n",
      "1334/1334 [==============================] - 0s 76us/step - loss: 0.4402 - acc: 0.8193 - val_loss: 0.4321 - val_acc: 0.8408\n",
      "Epoch 46/80\n",
      "1334/1334 [==============================] - 0s 71us/step - loss: 0.4361 - acc: 0.8208 - val_loss: 0.4281 - val_acc: 0.8408\n",
      "Epoch 47/80\n",
      "1334/1334 [==============================] - 0s 90us/step - loss: 0.4323 - acc: 0.8231 - val_loss: 0.4242 - val_acc: 0.8408\n",
      "Epoch 48/80\n",
      "1334/1334 [==============================] - 0s 91us/step - loss: 0.4285 - acc: 0.8231 - val_loss: 0.4203 - val_acc: 0.8408\n",
      "Epoch 49/80\n",
      "1334/1334 [==============================] - 0s 67us/step - loss: 0.4249 - acc: 0.8253 - val_loss: 0.4167 - val_acc: 0.8423\n",
      "Epoch 50/80\n",
      "1334/1334 [==============================] - 0s 73us/step - loss: 0.4214 - acc: 0.8283 - val_loss: 0.4131 - val_acc: 0.8438\n",
      "Epoch 51/80\n",
      "1334/1334 [==============================] - 0s 76us/step - loss: 0.4180 - acc: 0.8276 - val_loss: 0.4099 - val_acc: 0.8438\n",
      "Epoch 52/80\n",
      "1334/1334 [==============================] - 0s 82us/step - loss: 0.4149 - acc: 0.8276 - val_loss: 0.4066 - val_acc: 0.8453\n",
      "Epoch 53/80\n",
      "1334/1334 [==============================] - 0s 67us/step - loss: 0.4118 - acc: 0.8276 - val_loss: 0.4035 - val_acc: 0.8438\n",
      "Epoch 54/80\n",
      "1334/1334 [==============================] - 0s 79us/step - loss: 0.4088 - acc: 0.8291 - val_loss: 0.4007 - val_acc: 0.8483\n",
      "Epoch 55/80\n",
      "1334/1334 [==============================] - 0s 81us/step - loss: 0.4061 - acc: 0.8321 - val_loss: 0.3978 - val_acc: 0.8483\n",
      "Epoch 56/80\n",
      "1334/1334 [==============================] - 0s 68us/step - loss: 0.4035 - acc: 0.8298 - val_loss: 0.3951 - val_acc: 0.8498\n",
      "Epoch 57/80\n",
      "1334/1334 [==============================] - 0s 64us/step - loss: 0.4007 - acc: 0.8313 - val_loss: 0.3927 - val_acc: 0.8514\n",
      "Epoch 58/80\n",
      "1334/1334 [==============================] - 0s 106us/step - loss: 0.3982 - acc: 0.8321 - val_loss: 0.3901 - val_acc: 0.8529\n",
      "Epoch 59/80\n",
      "1334/1334 [==============================] - 0s 71us/step - loss: 0.3959 - acc: 0.8306 - val_loss: 0.3876 - val_acc: 0.8529\n",
      "Epoch 60/80\n",
      "1334/1334 [==============================] - 0s 85us/step - loss: 0.3935 - acc: 0.8321 - val_loss: 0.3851 - val_acc: 0.8529\n",
      "Epoch 61/80\n",
      "1334/1334 [==============================] - 0s 89us/step - loss: 0.3912 - acc: 0.8321 - val_loss: 0.3829 - val_acc: 0.8529\n",
      "Epoch 62/80\n",
      "1334/1334 [==============================] - 0s 89us/step - loss: 0.3892 - acc: 0.8358 - val_loss: 0.3808 - val_acc: 0.8544\n",
      "Epoch 63/80\n",
      "1334/1334 [==============================] - 0s 70us/step - loss: 0.3872 - acc: 0.8351 - val_loss: 0.3790 - val_acc: 0.8529\n",
      "Epoch 64/80\n",
      "1334/1334 [==============================] - 0s 68us/step - loss: 0.3854 - acc: 0.8351 - val_loss: 0.3773 - val_acc: 0.8529\n",
      "Epoch 65/80\n",
      "1334/1334 [==============================] - 0s 95us/step - loss: 0.3836 - acc: 0.8358 - val_loss: 0.3754 - val_acc: 0.8529\n",
      "Epoch 66/80\n",
      "1334/1334 [==============================] - 0s 91us/step - loss: 0.3817 - acc: 0.8373 - val_loss: 0.3735 - val_acc: 0.8529\n",
      "Epoch 67/80\n",
      "1334/1334 [==============================] - 0s 62us/step - loss: 0.3801 - acc: 0.8366 - val_loss: 0.3717 - val_acc: 0.8544\n",
      "Epoch 68/80\n",
      "1334/1334 [==============================] - 0s 62us/step - loss: 0.3783 - acc: 0.8396 - val_loss: 0.3699 - val_acc: 0.8574\n",
      "Epoch 69/80\n",
      "1334/1334 [==============================] - 0s 79us/step - loss: 0.3767 - acc: 0.8411 - val_loss: 0.3682 - val_acc: 0.8574\n",
      "Epoch 70/80\n",
      "1334/1334 [==============================] - 0s 110us/step - loss: 0.3752 - acc: 0.8403 - val_loss: 0.3667 - val_acc: 0.8589\n",
      "Epoch 71/80\n",
      "1334/1334 [==============================] - 0s 72us/step - loss: 0.3737 - acc: 0.8426 - val_loss: 0.3651 - val_acc: 0.8589\n",
      "Epoch 72/80\n",
      "1334/1334 [==============================] - 0s 89us/step - loss: 0.3723 - acc: 0.8433 - val_loss: 0.3637 - val_acc: 0.8589\n",
      "Epoch 73/80\n",
      "1334/1334 [==============================] - 0s 71us/step - loss: 0.3710 - acc: 0.8441 - val_loss: 0.3625 - val_acc: 0.8589\n",
      "Epoch 74/80\n",
      "1334/1334 [==============================] - 0s 83us/step - loss: 0.3697 - acc: 0.8441 - val_loss: 0.3614 - val_acc: 0.8604\n",
      "Epoch 75/80\n",
      "1334/1334 [==============================] - 0s 84us/step - loss: 0.3684 - acc: 0.8441 - val_loss: 0.3599 - val_acc: 0.8589\n",
      "Epoch 76/80\n",
      "1334/1334 [==============================] - 0s 95us/step - loss: 0.3673 - acc: 0.8463 - val_loss: 0.3586 - val_acc: 0.8589\n",
      "Epoch 77/80\n",
      "1334/1334 [==============================] - 0s 74us/step - loss: 0.3662 - acc: 0.8456 - val_loss: 0.3574 - val_acc: 0.8604\n",
      "Epoch 78/80\n",
      "1334/1334 [==============================] - 0s 63us/step - loss: 0.3648 - acc: 0.8493 - val_loss: 0.3563 - val_acc: 0.8604\n",
      "Epoch 79/80\n",
      "1334/1334 [==============================] - 0s 79us/step - loss: 0.3638 - acc: 0.8508 - val_loss: 0.3556 - val_acc: 0.8619\n",
      "Epoch 80/80\n",
      "1334/1334 [==============================] - 0s 81us/step - loss: 0.3628 - acc: 0.8501 - val_loss: 0.3547 - val_acc: 0.8559\n",
      "1998/1998 [==============================] - 0s 62us/step\n",
      "5000/5000 [==============================] - 0s 78us/step\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "\n",
    "batch_size = 100\n",
    "epochs = 80\n",
    "dropout_size = 0.5\n",
    "\n",
    "pca = PCA(n_components=n)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "X_4_full_pca = pca.transform(X_4_full[1000:, :])\n",
    "X_5_full_pca = pca.transform(X_5_full[1000:, :])\n",
    "    \n",
    "nav = []\n",
    "put = []\n",
    "for train, test in kfold.split(X_pca, y):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(250, input_dim=n, activation='relu'))\n",
    "    Dropout(dropout_size)\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    Dropout(dropout_size/2)\n",
    "    model.add(Dense(40, activation='relu'))\n",
    "    Dropout(dropout_size/3)\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_pca[train], y[train], batch_size=batch_size, epochs=epochs, shuffle=True, \n",
    "              validation_data=(X_pca[test], y[test]))\n",
    "    nav_scores = model.evaluate(X_4_full_pca, np.zeros(X_4_full_pca.shape[0]))\n",
    "    put_scores = model.evaluate(X_5_full_pca, np.ones(X_5_full_pca.shape[0]))\n",
    "    nav.append(round(nav_scores[1] * 100, 2))\n",
    "    put.append(round(put_scores[1] * 100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "\n",
    "X = np.vstack((X_4_full[0:1800, :], X_5_full[0:1800, :]))\n",
    "pca = PCA(n_components=n)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "X_4_full_pca = pca.transform(X_4_full[1800:, :])\n",
    "X_5_full_pca = pca.transform(X_5_full[1800:, :])\n",
    "y = np.append(np.zeros(1800), np.ones(1800))\n",
    "y_4 = np.zeros(X_4_full_pca.shape[0])\n",
    "y_5 = np.ones(X_5_full_pca.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as rfc\n",
    "from xgboost import XGBClassifier as xgb\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nav = []\n",
    "put = []\n",
    "\n",
    "param_grid = {'alpha' : [0.0001 ,0.001, 0.1, 1, 10]}\n",
    "scores = []\n",
    "\n",
    "for train, test in kfold.split(X_pca, y):\n",
    "    optimizer = GridSearchCV(RidgeClassifier(), param_grid, n_jobs=-1)\n",
    "    optimizer.fit(X_pca[train], y[train])\n",
    "    scores.append(optimizer.score(X_pca[test], y[test]))\n",
    "    nav.append(optimizer.score(X_4_full_pca, y_4))\n",
    "    put.append(optimizer.score(X_5_full_pca, y_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.80000000000000004, 0.79249999999999998, 0.77249999999999996]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.94999999999999996, 0.94125000000000003, 0.95250000000000001]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "put"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.75252525252525249, 0.76262626262626265, 0.75757575757575757]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nav = []\n",
    "put = []\n",
    "\n",
    "param_grid = {'n_estimators' : [10, 100, 300, 1000, 10000], 'max_depth' : [2, 3, 5, 7]}\n",
    "scores = []\n",
    "\n",
    "for train, test in kfold.split(X_pca, y):\n",
    "    optimizer = GridSearchCV(rfc(), param_grid, n_jobs=-1)\n",
    "    optimizer.fit(X_pca[train], y[train])\n",
    "    scores.append(optimizer.score(X_pca[test], y[test]))\n",
    "    nav.append(optimizer.score(X_4_full_pca, y_4))\n",
    "    put.append(optimizer.score(X_5_full_pca, y_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.80333333333333334, 0.80666666666666664, 0.8041666666666667]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.81313131313131315, 0.84343434343434343, 0.78282828282828287]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.91593749999999996, 0.91531249999999997, 0.93062500000000004]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "put"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 7, 'n_estimators': 300}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nav = []\n",
    "put = []\n",
    "\n",
    "param_grid = {'n_estimators' : [10, 100, 300, 1000], 'max_depth' : [2, 3, 5, 7], 'learning_rate' : [0.01, 0.05, 0.1, 0.3, 0.8, 2]}\n",
    "scores = []\n",
    "\n",
    "for train, test in kfold.split(X_pca, y):\n",
    "    optimizer = GridSearchCV(xgb(), param_grid, n_jobs=-1)\n",
    "    optimizer.fit(X_pca[train], y[train])\n",
    "    scores.append(optimizer.score(X_pca[test], y[test]))\n",
    "    nav.append(optimizer.score(X_4_full_pca, y_4))\n",
    "    put.append(optimizer.score(X_5_full_pca, y_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.79749999999999999, 0.80500000000000005, 0.8075]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.81313131313131315, 0.8232323232323232, 0.81313131313131315]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.91656249999999995, 0.92156249999999995, 0.92343750000000002]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "put"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1000}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/esthete/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/home/esthete/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/home/esthete/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/home/esthete/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/home/esthete/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/home/esthete/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/home/esthete/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/home/esthete/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/home/esthete/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/home/esthete/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/home/esthete/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/home/esthete/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "nav = []\n",
    "put = []\n",
    "\n",
    "param_grid = {'alpha' : [0, 0.5, 1]}\n",
    "scores = []\n",
    "\n",
    "for train, test in kfold.split(X_pca, y):\n",
    "    optimizer = GridSearchCV(BernoulliNB(), param_grid, n_jobs=-1)\n",
    "    optimizer.fit(X_pca[train], y[train])\n",
    "    scores.append(optimizer.score(X_pca[test], y[test]))\n",
    "    nav.append(optimizer.score(X_4_full_pca, y_4))\n",
    "    put.append(optimizer.score(X_5_full_pca, y_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.76833333333333331, 0.75083333333333335, 0.76083333333333336]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.71717171717171713, 0.71717171717171713, 0.73737373737373735]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.92156249999999995, 0.91531249999999997, 0.916875]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "put"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nav = []\n",
    "put = []\n",
    "\n",
    "scores = []\n",
    "\n",
    "for train, test in kfold.split(X_pca, y):\n",
    "    optimizer = GaussianNB()\n",
    "    optimizer.fit(X_pca[train], y[train])\n",
    "    scores.append(optimizer.score(X_pca[test], y[test]))\n",
    "    nav.append(optimizer.score(X_4_full_pca, y_4))\n",
    "    put.append(optimizer.score(X_5_full_pca, y_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.70166666666666666, 0.75749999999999995, 0.78166666666666662]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.88888888888888884, 0.67171717171717171, 0.81818181818181823]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.67437499999999995, 0.95250000000000001, 0.86843749999999997]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "put"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
